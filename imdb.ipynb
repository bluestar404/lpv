{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "924a95a9-31d6-4b4f-8da9-61fbccc62ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification using Deep Neural Networks Example: Classify movie\n",
    "#  reviews into positive\" reviews and \"negative\" reviews, just based on the text content of the reviews.Use\n",
    "#  IMDB datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c31aad-8c6d-4376-98bd-dfd61a8dcf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Importing NumPy, a package for numerical computing in Python (used for arrays, matrices, etc.)\n",
    "from keras.datasets import imdb  # Importing the IMDB dataset from Keras, which is pre-tokenized and used for sentiment analysis\n",
    "from keras import models, layers  # Importing Keras model and layers modules for defining and building the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776245a-431a-44e8-8ba2-510d43730f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Keras-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32fa925c-db2d-467d-88fa-09f66743ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset from Keras, considering only the top 10,000 most frequent words\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "# 'x_train' and 'x_test' contain the tokenized reviews (sequences of integers representing words)\n",
    "# 'y_train' and 'y_test' contain the corresponding labels (0 for negative sentiment, 1 for positive sentiment)\n",
    "# The 'num_words=10000' parameter ensures only the 10,000 most frequent words are considered, ignoring rare words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48c254-44a0-4096-af79-ccc7f8dece77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f0118-181c-46c9-8465-a58e662c72c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1ec4b-1641-46fc-83b3-4932036eac7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05dd1aa6-1934-4353-892d-d42ef00327d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequences of word indices into binary vector form\n",
    "def vectorize(sequences, dim=10000):\n",
    "    result = np.zeros((len(sequences), dim))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        result[i, seq] = 1\n",
    "    return result\n",
    "\n",
    "x_train = vectorize(x_train)\n",
    "x_test = vectorize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ffedf1c-07c6-4ccc-b3d1-2700480fad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define a simple deep neural network model\n",
    "model = models.Sequential([  # Sequential model: stack layers linearly\n",
    "    layers.Input(shape=(10000,)),  # Explicit input layer: expects input with 10,000 features (e.g., tokenized and padded text)\n",
    "    \n",
    "    layers.Dense(16, activation='relu'),  # Fully connected layer with 16 neurons and ReLU activation\n",
    "    # ReLU (Rectified Linear Unit) activation allows for non-linearity and helps with training deep networks.\n",
    "\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer with 1 neuron for binary classification (0 or 1)\n",
    "    # Sigmoid activation ensures the output is a probability between 0 and 1, suitable for binary classification.\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a11bf873-6cac-40b1-b612-d24ad0f36bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with appropriate optimizer, loss function, and metric\n",
    "model.compile(\n",
    "    optimizer='adam',  # Adam optimizer: adaptive learning rate optimization algorithm that combines the benefits of other optimizers\n",
    "    loss='binary_crossentropy',  # Loss function: binary cross-entropy is used for binary classification problems (0 or 1 labels)\n",
    "    metrics=['accuracy']  # Metric: accuracy is used to evaluate how many predictions match the true labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87d083de-4161-41dd-a5f2-91373db769f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7083 - loss: 0.5897 - val_accuracy: 0.8748 - val_loss: 0.3636\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9019 - loss: 0.3049 - val_accuracy: 0.8850 - val_loss: 0.2997\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9266 - loss: 0.2323 - val_accuracy: 0.8882 - val_loss: 0.2804\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9382 - loss: 0.1936 - val_accuracy: 0.8920 - val_loss: 0.2746\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9491 - loss: 0.1661 - val_accuracy: 0.8876 - val_loss: 0.2783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fb1a4d38b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on training data with a validation split\n",
    "model.fit(\n",
    "    x_train,  # Training data (input features)\n",
    "    y_train,  # Training labels (target values)\n",
    "    epochs=5,  # Number of times the entire dataset is passed through the model during training\n",
    "    batch_size=512,  # Number of samples per gradient update (how many samples to process before updating model weights)\n",
    "    validation_split=0.2  # 20% of the training data will be used for validation during training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0fda72b-c0d0-418f-9274-30cc5fb30209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8833 - loss: 0.2899\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18de886-1bf2-46e3-8077-9bbd8eac1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model achieved an accuracy of 88.33%, meaning it correctly predicted the\n",
    "# class (e.g., positive or negative sentiment) for 88.33% of the test or validation samples. \n",
    "# The loss of 0.2899 represents how far the model's predictions \n",
    "# are from the actual labels, with lower loss indicating better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84f4d5-8d91-4e03-a4c8-3db793871c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
